{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86ac230-960c-4534-a502-5bde607b0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhor/miniconda3/envs/materials/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from haystack import Document\n",
    "from haystack.nodes import FARMReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b620581-7594-4836-b5e0-bbc382deffd5",
   "metadata": {},
   "source": [
    "## Load a reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67034e02-2147-49ad-8807-305bcbadb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2484151/3210507208.py:2: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_3.txt' mode='r' encoding='UTF-8'>\n",
      "  paragraphs = open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "file = \"reference_txts/reference_3.txt\"\n",
    "paragraphs = open(file, 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd54a18-a39f-4ef0-8592-72db44aceb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.  Introduction\\n',\n",
       " 'In  general,  structural  materials  that  are  currently  available  for most  of  the  applications  are  based  on  single  principle  elements viz.  Fe-based  steels,  Al-based  alloys,  Ni-based  superalloys,  etc.\\n',\n",
       " 'Although,  solid  solution  strengthening  by  more  than  one  solute  in equi-molar  proportion  is  a  viable  option,  processing  of  alloys  based on  multi-principal  elements  was  not  attempted  for  a  long  time on  the  notion  that  they  may   form  complex  compounds  with  complex  crystal  structures.  However,  the  ﬁrst  impressive  report  by  Yeh et  al.  [1],   followed  by  a  few  research  groups  [2–5],  has  emphatically shown  that  multi-component  high  entropy  alloys  (HEAs)  can  be processed  to  form  simple  solid  solutions  under  appropriate  conditions.  The  term  “high  entropy”  in  HEAs  originates  from  the  resultant high  entropy  of  mixing  (>1.61R,  where  R  is  gas  constant)  when more  than  ﬁve  elements  are  mixed  in  equi-molar  proportion.  The HEAs  formed  with  simple  crystal  structures  have  also  exhibited interesting  properties  such  as  high  room  temperature  strength,  and strength  retention  at  higher  temperatures  [1,2,5].\\n',\n",
       " 'There  have  been  several  processing  routes  to  prepare  HEAs, viz.,  casting,  sputtering,  splat  quenching,  mechanical  alloying  (MA), and  among  those  processes,  HEAs  prepared  by  casting  route  are widely  studied.  Although,  HEAs  prepared  by  casting  nominally\\n',\n",
       " 'Multi-component   high   entropy   alloys   (HEAs)   are   observed   to  form   simple   solid   solutions   in   contrary   to general   perception   that   complex   compounds   may   form   in   such   multi-component   equi-atomic   alloys.   In the   present   study,   alloying   behavior   was   investigated   using   XRD   in   AlCoCrCuFe   and   NiCoCrCuFe   equiatomic   high   entropy   alloys   synthesized   by   mechanical   alloying   (MA)   and   spark   plasma   sintering   (SPS).\\n',\n",
       " 'Simple   FCC   and   BCC   phases   evolved   after   MA,   while   Cu-rich   FCC   and   sigma   ((cid:2))  phases   evolved   along with   FCC   and  BCC   phases   after   SPS.   Further,   NiCoCuFe,   NiCoCrFe   and   NiCoFe   equi-atomic   alloys   were investigated   to  conﬁrm   the  formation   of   Cu-rich   FCC,   and     phases.   The   hardness   was   observed   to  be 770  \\n',\n",
       " '  10   HV   for   NiCoCrCuFe.   Phase   evolution   after   MA   and   SPS   indicate (cid:2)that   conﬁgurational   entropy   is   not   sufﬁcient   enough   to  suppress   the   formation   of   Cu-rich   FCC,   and   phases,   and   enthalpy   of   mixing   appears   to   play   an   important   role   in   determining   the   phase   formation   in high   entropy   alloys   after   sintering.\\n',\n",
       " '  10   HV   for   AlCoCrCuFe   and   400   (cid:2)show  simple  solid  solutions,  detailed  analyses  have  shown  conclusively  that  casting  route  usually  results  in  typical  dendrite  structure with  some  segregation  in  dendrite  and  inter-dendrite  regions  [3–7].\\n',\n",
       " 'Similarly,  metastable  phases  relative  to  casting  are  formed  during splat  quenching  or  sputtering  method,  and  reported  that  these  may become  stable  after  annealing  at  high  temperature  [3].  On  the  contrary,  HEAs  formed  via  MA   route  appear  to  give  more  homogeneous and  stable  nanocrystalline  microstructures  [8–14].  Further,  good densiﬁcation  characteristics  and  high  hardness  were  observed  for HEA  synthesized  by  MA   and  spark  plasma  sintering  (SPS)  [13].\\n',\n",
       " 'Based  on  the  contradicting  observations  in  the  literature  on alloying,  the  persistent  question  one  needs  to  ask  is  whether  solid solution  criteria  proposed  for  HEAs  in  the  literature  [15,16]  are valid  for  MA   route.  It  is  also  important  to  know  why  some  single  phase  HEAs  formed  after  MA,   phase-separate  (or  precipitate other  phases)  during  densiﬁcation  [12,13].  In  order  to  address  these issues,  in  the  present  study,  equi-atomic  AlCoCrCuFe,  NiCoCrCuFe, NiCoCuFe,  NiCoCrFe  and  NiCoFe  were  chosen  for  synthesis  of  HEAs by  MA   followed  by  densiﬁcation.  Alloying  behavior  of  the  above alloys  during  MA   and  after  densiﬁcation  is  studied  by  employing  a detailed  characterization  via  X-ray  diffraction  (XRD)  and  scanning electron  microscopy  (SEM).\\n',\n",
       " '2.  Experimental  details\\n',\n",
       " 'Equi-atomic  elemental  blends  of  Al,  Co,  Cr,  Cu,  Fe,  Ni  powders with  purity  higher  than  99.5%  were  mechanically  alloyed  to  synthesize  equi-atomic  AlCoCrCuFe,  NiCoCrCuFe,  NiCoCrFe,  NiCoCuFe 0921-5093/$  –  see  front  matter ©  \\x0c84\\n',\n",
       " 'S.  Praveen  et  al.  /  Materials  Science  and  Engineering  A  534 (2012) 83–  89 and  NiCoFe  HEAs.  FritshPulverisette-P5  high  energy  ball  mill  at  a ball  to  powder  weight  ratio  of  10:1  using  hard  chrome  steel  vials and  balls  was  utilized  for  mechanical  alloying.  Ball  milled  powders  were  consolidated  by  SPS  at  900 ◦C  for  15  min   at  a  pressure of  50  MPa.  To  study  the  alloying  behavior  during  milling,  powder samples  were  taken  at  a  regular  interval  of  5  h  and  XRD  experiments  were  carried  out  in  Xpert  Pro  Panalytical  instrument.  Phase evolution  after  sintering  was  also  studied  by  XRD.  Hardness  measurements  were  carried  out  on  the  consolidated  samples  at  a  load of  1  kg  with  dwell  time  of  10  s  in  Wilpert  WilsonVicker’s  hardness instrument.  The  reported  hardness  measurements  are  an  average of  at  least  ten  measurements  done  on  both  sides  of  the  sample pellet.  A  SEM  equipped  with  energy  dispersive  spectrometry  (EDS) was  used  for  phase  identiﬁcation  and  composition  analysis  of  the phases.  In  order  to  verify  the  homogeneity  of  the  dense  pellets,  SEM and  hardness  measurements  were  carried  out  on  both  sides  of  the pellet  for  each  alloy  system.\\n',\n",
       " '3.  Results\\n',\n",
       " 'The  XRD,  SEM  micrographs  and  hardness  measurementsreported  here  are  the  representative  results  of  a  given  composition.  All  the  results  (XRD,  SEM  and  hardness)  presented  here  are observed  to  be  consistent  on  both  sides  of  the  pellets,  and  thus ensuring  the  homogeneity  and  uniformity  of  the  dense  pellets.\\n',\n",
       " 'Fig.  1a  and  b  represents  the  XRD  patterns  of  the  mechanically alloyed  powders  of  AlCoCrCuFe  and  NiCoCrCuFe  as  a  function  of milling  time,  respectively.  Major  BCC  (designated  as  B)  phase  and  a minor  FCC  phase  were  observed  in  AlCoCrCuFe  after  15  h  of  milling.\\n',\n",
       " 'A  closer  look  at  the  peak  positions  of  the  minor  FCC  phase  indicates that  this  FCC  phase  is  Cu  rich  and  has  been  designated  as  FCu.  In contrast,  Cu  dissolves  completely  in  NiCoCrCuFe,  and  a  major  FCC (designated  as  F)  phase  evolves  together  with  minor  BCC  phase (designated  as  Bminor).  It  is  clear  from  Fig.  1b  that  in  NiCoCrCuFe,  Ni peaks  shift  towards  lower  angles  (towards  Cu  peaks)  with  milling time  (compare  the  XRD  of  10  min   and  10  h  milling)  and  the  ﬁnal FCC  (F)  phase  peak  positions  are  close  to  that  of  Cu.\\n',\n",
       " 'Subsequently,  the  afﬁnity  for  FCC  phase  formation  was   veriﬁed  by  choosing  two  different  compositions,  NiCoFe  and  NiCoCrFe without  Cu,  and  another  composition  NiCoCuFe  with  Cu.  Fig.  2a shows  the  XRD  patterns  of  NiCoFe,  NiCoCrFe  and  NiCoCuFe  together with  NiCoCrCrFe.  Among  these  three  alloys,  NiCoFe  and  NiCoCuFe have  shown  single  phase  FCC  (F)  formation,  while  the  Cr  containing  NiCoCrFe  has  shown  a  small  amount  of  BCC  phase  (Bminor)  along with  FCC  (F)  on  MA,   which  is  clear  from  the  asymmetry  in  the  lowest  angle  XRD  peak  (Fig.  2a).  Fig.  2b  and  c  shows  the  ﬁrst  two   XRD peaks  of  NiCoCrFe  and  NiCoCrCuFe  in  de-convoluted  form,  which illustrate  that  a  minor  BCC  phase  (Bminor)  forms  in  Cr  containing alloys.\\n',\n",
       " 'It  is  interesting  to  note  from  the  XRD  patterns  of  NiCoCuFe  and NiCoCrCuFe  (Fig.  2a)  that  the  FCC  phase  (F)  peak  positions  are  very close  to  those  of  Cu  in  both  the  alloys  containing  Cu.  This  observation  suggests  that  all  the  other  elements  appear  to  be  dissolving into  Cu.  Interestingly,  XRD  patterns  of  NiCoFe  and  NiCoCrFe  show that  the  FCC  (F)  peak  positions  in  these  alloys  are  also  same  as  in  the alloys  with  Cu,  though  these  alloys  do  not  contain  Cu.  This  clariﬁes that  all  other  elements  are  dissolving  in  Ni  and  not  in  Cu  and  thus the  peak  shift  of  Ni  is  not  because  of  formation  of  FCC  (F)  phase  with Cu.  It  is  also  important  to  note  that  when  Al  replaces  Ni  to  form  an alloy  such  as  AlCoCrCuFe  (Fig.  1a),  all  the  elements  except  Cu  are observed  to  be  dissolving  into  Fe  or  Cr  and  no  shift  in  peaks  was observed  with  milling  time.\\n',\n",
       " 'Fig.  1.  XRD  patterns  of  mechanically  alloyed  powders  of  (a)  AlCoCrCuFe  and  (b) NiCoCrCuFe  HEAs  as  a  function  of  milling  time.\\n',\n",
       " '3.2.  Phase  evolution  due  to  densiﬁcation  by  SPS Fig.  3  shows  the  XRD  pattern  of  AlCoCrCuFe  and  NiCoCrCuFe after  SPS  at  900 ◦C.  A  detailed  analysis  of  the  XRD  pattern  of  AlCoCrCuFe  after  sintering  illustrates  that  a  major  ordered  BCC  phase  (B2), a  Cu  rich  FCC  phase  (FCu),  and  minor  sigma  phase  ((cid:2))  are  visible.  It is  important  to  note  that  in  contrast  to  only  one  BCC  (B)  phase  with partial  dissolution  of  Cu  after  15  h  ball-milling,  additional  phase  ((cid:2)) evolved  during  sintering.  The  Cu  is  observed  to  segregate  out  as  an FCC  phase  (FCu)  and  accordingly  diffraction  peaks  corresponding  to Cu  are  clearly  visible.\\n',\n",
       " 'Inset  in  Fig.  3  shows  the  prominent  XRD  peaks  of  NiCoCrCuFe  in de-convoluted  form  illustrating  the  formation  of  two   FCC  phases  (F1 and  F2)  after  SPS.  As  compared  to  only  one  FCC  phase  (F)  and  a  minor BCC  (Bminor)  after  15  h  ball-milling  in  NiCoCrCuFe  (Fig.  1b),  two   FCC phases  (F1 and  F2),  and  minor  sigma  phase  ((cid:2))  are  observed  after SPS  in  NiCoCrCuFe  (Fig.  3).  Unlike  AlCoCrCuFe,  no  ordered  BCC  (B2) phase  was   observed  in  NiCoCrCuFe  after  sintering.  Fig.  4  shows  the XRD  pattern  of  NiCoFe,  NiCoCrCuFe,  NiCoCrFe,  and  NiCoCrCu  after SPS  at  900 ◦C;  inset  shows  the  de-convoluted  form  of  XRD  peaks.\\n',\n",
       " 'Formation  of  the  second  FCC  phase  (F2)  in  Cu  containing  systems (NiCoCuFe,  NiCoCrCuFe,  AlCoCrCuFe)  indicates  that  the  presence  of Cu  led  to  the  segregation  of  second  FCC  phase  (F2).  Absence  of  minor \\x0cS.  Praveen  et  al.  /  Materials  Science  and  Engineering  A  534 (2012) 83–  89 85\\n',\n",
       " 'Fig.  2.  (a)  XRD  patterns  of  mechanically  alloyed  HEAs,  (b)  and  (c)  enlarged  part  of  XRD  patterns  showing  de-convoluted  peaks.\\n',\n",
       " 'Fig.  3.  XRD  patterns  of  AlCoCrCuFe  and  NiCoCrCuFe  after  SPS.  Inset  shows  the  deconvoluted  form  of  XRD  peaks  of  NiCoCrCuFe  showing  the  formation  of  F1 and  F2.\\n',\n",
       " 'Fig.  4.  XRD  patterns  of  NiCoFe,  NiCoCuFe,  NiCoCrFe  and  NiCoCrCuFe  after  SPS.  Inset shows  the  de-convoluted  form  of  XRD  peaks  corresponding  to  NiCoCrCuFe.\\n',\n",
       " 'S.  Praveen  et  al.  /  Materials  Science  and  Engineering  A  534 (2012) 83–  89 Table  1\\n',\n",
       " 'Phase  evolution  after  MA   and  SPS  based  on  XRD  analysis.\\n',\n",
       " 'Crystallite  size  and  lattice  strain  of  MA   and  SPS  samples.\\n',\n",
       " 'After  MA  \\n',\n",
       " 'BCC  (B)  and  Cu  rich  FCC (FCu)\\n',\n",
       " 'FCC  (F)  and  BCC  (Bminor)  FCC  (F)  and  BCC  (Bminor)  FCC  (F)  FCC  (F) \\n',\n",
       " 'Ordered  BCC  (B2),  Cu  rich FCC  (FCu),  sigma  phase  ((cid:2)) Two   FCC  (F1 and  F2),  sigma phase  ((cid:2))\\n',\n",
       " 'FCC  (F)  and  sigma  phase  ((cid:2)) Two   FCC  (F1 and  F2) FCC  (F)\\n',\n",
       " 'Relative  density  obtained  for  the  SPS  samples.\\n',\n",
       " 'Vickers  hardness  for  HEAs.\\n',\n",
       " 'Experimental  relative density  (%) 6.75  8.33  8.17  8.64  8.53  99 94 94 86 92sigma  phase  ((cid:2))  in  NiCoFe  and  NiCoCuFe  suggests  that  the  minor sigma  phase  ((cid:2))  in  NiCoCrCuFe  and  NiCoCrFe  is  a  Cr  containing phase.  A  detailed  summary  of  phases  formed  after  MA   and  SPS  are listed  in  Table  1  for  comparison\\n',\n",
       " 'Fig.  5a  and  b  represents  back-scattered  electron  (BSE)  images  of AlCoCrCuFe  at  different  magniﬁcations  after  densiﬁcation  by  SPS, and  Fig.  5c  and  d  represents  those  of  NiCoCrCuFe.  In  both  these  alloy systems,  a  clear  two-phase  contrast  is  visible  from  BSE-SEM  micrographs.  Fig.  6a  and  b  illustrates  the  spot  EDS  analysis  on  the  two phases  (white  and  grey)  in  AlCoCrCuFe,  and  the  light  grey  and  dark grey  phases  in  NiCoCrCuFe.  Black  spots  scattered  in  microstructure correspond  to  micro  and  nano  porosity.\\n',\n",
       " 'The  BSE-SEM  micrographs  (Fig.  5a  and  b)  together  with  spot  EDS analysis  (Fig.  6a)  suggest  that  white  phase  is  a  Cu-rich  phase  and grey  phase  is  a  Cu-depleted  phase  in  AlCoCrCuFe.  Irrespective  of Cu  distribution,  other  elements  are  presented  at  near  equi-atomic.\\n',\n",
       " 'Clearly,  this  Cu-rich  phase  (white)  can  also  be  correlated  with  the FCC  (FCu)  phase,  and  Cu-depleted  phase  (grey)  with  major  BCC  (B2) phase  observed  in  XRD  pattern  (Fig.  3).  Spot  EDS  analysis  (Fig.  6b) on  NiCoCrCuFe  indicates  that  dark  grey  phase  is  Cr-rich  phase  while light  grey  phase  is  Cr-depleted  phase.  Atomic  percentage  of  Ni and  Cu  observed  to  be  more  in  Cr  depleted  phase  than  Fe  and Co.  However,  Ni,  Fe  Co  and  Cu  are  observed  to  be  distributed  at near  equi-atomic  percentage  in  Cr  rich  phase.  At  lower  magniﬁcation,  as  shown  in  Fig.  5c,  Cr-depleted  light  grey  phase  seems  to  be distributed  as  a  network  like  structure.  However,  at  higher  magniﬁcation  (Fig.  5d),  Cr-rich  phase  is  distributed  within  the  Cr-depleted phase.  The  Cr-depleted  phase  (light  grey)  can  be  correlated  with that  of  FCC  (F2)  phase  of  XRD  (Fig.  3),  and  Cr-rich  phase  (dark  grey) with  that  of  FCC  (F1)  and  the  small  amount  of    phase  observed  in\\n',\n",
       " 'XRD  pattern  (which  is  not  clearly  visible  in  the  SEM  images).\\n',\n",
       " 'Theoretical  density  of  the  alloys  was  calculated  using  rule  of mixture  of  elemental  densities  and  experimental  density  was   measured  using  Archimedes  principle.  Relative  density  (%)  is  calculated with  respect  to  theoretical  density  and  reported  in  Table  2.  AlCoCrCuFe  shows  a  high  relative  density  of  99%.  However,  when  Al  was replaced  by  Ni,  the  alloy  shows  a  decrease  in  relative  density  to  94% under  same  parameters  of  sintering.  Also,  higher  densiﬁcation  was observed  in  Cr  containing  system  than  systems  without  Cr.\\n',\n",
       " '3.4.  Crystallite  size  and  Vickers  hardness\\n',\n",
       " 'Pseudo-Voigt  function  was   used  for  ﬁtting  XRD  peak  proﬁle  and Si  standard  was  used  for  instrumental  broadening  correction.  Crystallite  size  and  lattice  strain  were  calculated  for  the  consolidated sample  using  Williamson–Hall  method.  XRD  pattern  proﬁle  ﬁtting, size  and  strain  calculation  were  done  using  Xpert  Highscore  plus software.  Table  3  presents  the  crystallite  size  and  lattice  strain  for mechanical  alloyed  powders  and  spark  plasma  sintered  samples estimated  using  the  Williamson–Hall  analysis.  Crystallite  sizes  of 6–10  nm  and  lattice  strain  of  0.5–1.0%  are  observed  for  mechanical  alloyed  powders.  However,  after  SPS,  lattice  strain  was   reduced to  less  than  0.2%  and  some  growth  in  crystallite  size  was   observed while  retaining  the  nanocrystallanity.  Growth  in  crystallite  size  is observed  to  be  higher  in  AlCoCrCuFe  (10–74  nm)   than  in  other  alloy systems.\\n',\n",
       " 'Vicker’s  hardness  values  for  the  spark  plasma  sintered  samples  are  shown  in  Table  4.  The  highest  hardness  of  770    10  HV  isobserved  for  AlCoCrCuFe.  When  Al  is  replaced  by  Ni,  hardness  drops drastically  to  400  ±  10  HV.  Further,  the  removal  of  Cu  from  NiCoCrCuFe  brought  up  the  hardness  value  to  490    10  HV.  However,  when\\n',\n",
       " 'Cr  is  absent  from  NiCoCrCuFe  and  NiCoCrFe,  the  hardness  value  is observed  to  be  very  less,  160    5  HV\\n',\n",
       " '  10  HV  for  NiCoCuFe  and  230  4.  Discussion\\n',\n",
       " '4.1.  Anomalies  with  solid  solution  criteria  for  HEAs The  excellent  work  by  Yeh  et  al.  [1–6]  has  established  that  it is  possible  to  form  simple  solid  solutions  when  ﬁve  or  more  than ﬁve  elements  are  mixed  in  equi-atomic  proportion.  Subsequently, Zhang  and  Zhou  [15,16]  have  proposed  a  solid  solution  criteria for  multi-principle  elemental  HEAs.  It’s  been  suggested  that  multicomponent  equi-atomic  HEAs  are  expected  to  form  single  phase solid  solution,  if  the  following  conditions  are  satisﬁed:  conﬁgurational  entropy  ((cid:2)Sconﬁg)  greater  than  or  equal  to  1.61R  (R  is universal  gas  constant),  enthalpy  of  mixing  ((cid:2)Hm)  in  the  range {(−2.685ı)  −15 of \\n',\n",
       " '  =  4.6%),  and  atomic  size  difference  (ı)  less  than  or  equal  to  4.6% for \\n',\n",
       " '[15,16].  Thus,  these  parameters  are  calculated  for  the  alloy  systems in  the  present  study  and  are  listed  in  Table  5.  It  is  important  to  note that  solid  solution  criteria  speciﬁed  by  Zhang  and  Zhou  are  satisﬁed by  all  the  alloy  systems  studied  in  the  present  work.  However,  as summarized  in  Table  1,  at  least  two   phases  are  observed  in  all  the {(−1.28ı)  +  5.44  kJ/mol} (cid:2)Hm <    (i.e.  5  to    2.54}   < \\n',\n",
       " '\\x0cS.  Praveen  et  al.  /  Materials  Science  and  Engineering  A  534 (2012) 83–  89 87\\n',\n",
       " 'Fig.  5.  The  BSE-SEM  images  of  SPS  pellets:  (a)  and  (b)  of  AlCoCrCuFe  and  (c)  and  (d)  of  NiCoCrCuFe.\\n',\n",
       " 'Fig.  6.  Spot  EDS  analysis  results  of  different  regions  in  (a)  AlCoCrCuFe  and  (b)  NiCoCrCuFe.\\n',\n",
       " 'Solid  solution  criteria  parameter  values  for  HEAs.\\n',\n",
       " 'Enthalpy  of  mixing  (kJ/mol)  between  elements.\\n',\n",
       " 'Entropy  of  mixing ((cid:2)Smix)\\n',\n",
       " '(R  =  8.314  J/mol  K) Enthalpy  of  mixing (kJ/mol)  ((cid:2)Hmix) Atomic  sizealloys,  except  in  NiCoFe.  Thus,  it  is  anticipated  that  the  solid  solution  criteria  speciﬁed  for  multi-principle  elemental  alloys  appear to  be  a  necessary  condition  but  not  sufﬁcient  enough  to  predict  the single  phase  solid  solution  formation  in  HEAs.\\n',\n",
       " 'In  addition,  Cu  is  observed  to  precipitate  out  and  such  an  observation  is  also  consistent  with  earlier  reports  dealing  with  the  alloy systems  containing  Cu  [3,4,6,7].  The  possible  reasons  for  Cu  segregation  can  be  explained  based  on  the  enthalpy  of  mixing  of  Cu with  other  elements  in  a  given  alloy  system.  Table  6  presents  the \\x0c88\\n',\n",
       " 'S.  Praveen  et  al.  /  Materials  Science  and  Engineering  A  534 (2012) 83–  89 enthalpy  of  mixing  of  the  equi-atomic  binary  pairs  of  elements  used in  this  study.\\n',\n",
       " 'It  is  illustrated  clearly  in  Table  6  that  as  compared  to  all  other  elements  in  the  present  alloy  systems,  Cu  has  positive  enthalpy  mixing with  more  number  of  elements  (with  Co,  Cr  and  Fe).  According  to the  so-called  deﬁnition  of  HEAs  [1]  and  based  on  solid  solution criteria  for  HEAs  [15,16],   simple  FCC  phase  or  BCC  phase  with  no intermediate  phase  is  expected  to  form  in  the  present  alloy  systems.  However,  in  addition  to  Cu-rich  phase  precipitation  during densiﬁcation,  sigma  phase  is  also  observed  in  all  the  alloy  systems containing  Cr.  This  sigma  phase  could  be  CoCr  or  FeCr  intermediate  phase  with  tetragonal  structure  and  it  was   documented  to be  observed  at  the  center  of  the  phase  diagrams  of  Cr–Co  and Fe–Cr  [17–19].   Previous  studies  on  Al0.5CoCrCuFeNiTix (x  =  0–2), AlCoxCrFeMo0.5 (x  =  0.5–2)  and  AlCoCrxFeMo0.5Ni  have  also  shown the  formation  of  Co–Cr    phase  [6,20,21].\\n',\n",
       " 'In  the  present  study,  NiCoFe  system  has  the  least  conﬁgurational entropy  among  the  ﬁve  alloy  systems  studied,  however,  a  single phase  FCC  solid  solution  has  formed  after  MA   in  this  alloy,  and this  phase  is  stable  even  after  sintering.  This  observation  together with  afore-mentioned  discussion  suggest  that  higher  or  lower  conﬁgurational  entropy  alone  cannot  justify  the  formation  of  single phase  solid  solution  in  HEAs.  Wang  et  al.  [22]  also  made  similar observation,  and  reported  that  conﬁgurational  entropy  alone  is  not sufﬁcient  enough  to  suppress  the  formation  of  Al–Ni  intermetallic compound  in  AlCrFeCoNiCu.  Also,  in  the  present  study  intermetallic  B2  phase  formation  could  not  be  suppressed  in  AlCoCrCuFe alloy  after  SPS,  although  the  alloy  has  large  conﬁgurational  entropy due  to  equi-atomic  quinary  composition.  Thus,  there  is  a  need  for devising  an  appropriate  and  valid  solid  solution  criteria  for  multiprincipal  HEAs,  and  the  existing  criteria  are  insufﬁcient  to  explain the  present  results.\\n',\n",
       " '4.2.  Possible  explanation  for  speciﬁc  phase  formation  in  each system\\n',\n",
       " 'In  NiCoCrCuFe  alloy  (Fig.  1b),  a  shift  in  the  Ni  peaks  towards lower  angles  during  MA   is  observed  and  all  the  elements  are observed  to  be  dissolving  into  Ni  peaks  with  increasing  milling time.  The  shift  in  the  peak  can  be  attributed  to  alloy  formation  initiated  after  10  h  of  milling.  The  FCC  being  a  closed  pack  structure, it  may   not  be  able  to  accommodate  other  elements  without  lattice expansion,  and  thus  a  shift  in  Ni  peaks  towards  left.  A  similar  behavior  has  been  observed  by  Gomez-Esparza  et  al.  in  NiCoAlFeCu  [14].\\n',\n",
       " 'Lattice  parameter  of  Ni  and  Cu  are  3.52  and  3.61 ˚A,  respectively.\\n',\n",
       " 'The  lattice  parameter  of  NiCoFe,  NiCoCuFe,  NiCoCrFe  and  NiCoCrCuFe  estimated  from  the  respective  XRD  patterns  are  3.58,  3.60, 3.59  and  3.62 ˚A,  respectively.  A  small  increase  in  lattice  parameter can  be  seen  with  increasing  the  number  elements,  which  clearly suggests  the  lattice  expansion  and  shift  in  Ni  peaks  towards  left with  increasing  the  number  of  elements.  In  contrast,  no  peak  shift is  observed  in  AlCoCrCuFe  as  the  milling  progresses,  and  all  the  elements  except  Cu  are  observed  to  be  dissolving  into  Fe  or  Cr  peaks.\\n',\n",
       " 'It  is  possible  that  Fe  and  Cr  having  more  open  BCC  crystal  structure,  they  are  able  to  accommodate  other  elements  without  much expansion.  In  addition,  it  is  also  possible  that  some  elements  might expand,  while  others  might  contract  the  lattice  and  hence  the  lattice  parameter  may   not  show  appreciable  change.  In  addition,  the fundamental  peak  positions  of  B2  phase  are  very  close  to  the  Fe/Cr peak  positions  and  hence  the  BCC  phase  observed  after  MA   could be  disordered  B2  phase,  which  after  SPS  reorders  and  shows  the superlattice  reﬂections.  This  reordering  is  caused  by  the  annihilation  of  defects  introduced  into  the  structure  due  to  high-energy  ball milling.\\n',\n",
       " 'Complete  solubility  of  Cu  is  not  observed  in  AlCoCrCuFe  after MA,   and  subsequently  Cu  segregates  out  as  a  Cu-rich  FCC  (FCu) phase  during  sintering.  As  explained  in  detail  in  Section  4.1,   the high  positive  enthalpy  of  mixing  of  Cu  with  other  alloying  elements could  be  the  primary  reason  for  such  behavior.  In  contrast,  when Al  was   replaced  by  Ni,  complete  solubility  of  Cu  into  the  FCC  (F) phase  was  observed  during  MA.   This  could  be  attributed  to  the  fact that  Cu  and  Ni  have  complete  solid  solubility.  Complete  segregation  of  Cu  after  sintering  is  not  observed  in  NiCoCrCuFe.  The  Cr depleted  FCC  phase  (F2)  has  more  at.%  of  Cu  followed  by  Ni,  Co  and Fe  are  observed.  Despite  positive  enthalpy  of  mixing  of  Cu  with other  elements,  strong  afﬁnity  of  Cu  and  Ni  prevents  the  complete segregation  of  Cu  into  the  second  FCC  phase  (F2)  after  sintering,  in contrast  to  Cu-rich  FCC  phase  (FCu)  in  AlCoCrCuFe.  Thus,  in  the  NiCoCrCuFe  alloy,  the  F1 phase  could  be  the  Fe–Ni–Cr  FCC  phase  (as observed  in  Ni  containing  steels  as  Ni  is  FCC  austenite  stabilizer) with  some  amount  of  Co  and  Cu  in  it.  The  F2 phase  in  these  alloys could  be  the  Cu–Ni  FCC  phase.\\n',\n",
       " 'Removal  of  Cr  or  both  Cr  and  Cu  in  NiCoCrCuFe  leads  to  formation  of  single  FCC  (F)  phase  after  MA.   However,  the  presence of  Cu  or  Cr  led  to  the  formation  of  new  FCC  phase  (FCu)  or  sigma phase  ((cid:2))  after  sintering,  respectively.  This  observation  suggests that  MA   being  a  non-equilibrium  process  facilitates  the  formation  of  metastable  FCC  phase.  Consequently,  densiﬁcation  at  higher temperatures  has  resulted  in  more  stable  phases  by  segregation  of Cu  and  Cr  into  either  Cu  rich  phase  FCC  phase  or  Cr  containing  sigma phase,  depending  on  the  composition.\\n',\n",
       " 'As  explained  in  Section  3,  MA   resulted  in  the  formation  of nanocrystalline  phases.  It  is  also  interesting  to  note  that  even  after the  sintering,  the  crystallite  growth  is  not  very  signiﬁcant  (<30  nm) in  all  the  alloys  except  in  AlCoCrCuFe.  In  this  connection,  it  is  important  to  note  that  FCC  is  the  major  phase  in  all  the  systems  except in  AlCoCrCuFe.  Crystal  growth  or  grain  growth  is  governed  by  diffusion  kinetics  in  a  given  system.  In  general,  BCC  being  an  open structure,  diffusion  takes  place  easily  as  compared  to  close  packed structure  of  FCC.  Thus,  crystal  growth  in  Ni  containing  alloys  (FCC phases)  is  sluggish  in  contrast  to  BCC  structure  of  AlCoCrCuFe.\\n',\n",
       " 'Although  crystallite  size  of  AlCoCrCuFe  is  higher  among  the alloys  studied,  it  exhibits  the  highest  hardness.  There  can  be a  combination  of  factors  contributing  to  such  a  higher  hardness  in  AlCoCrCuFe.  Presence  of  ordered  BCC  (B2)  phase  together with  highest  density  and  nanocrystallinity  in  AlCoCrCuFe  must  be responsible  for  exhibiting  higher  hardness.  One  of  the  reasons  for this  alloy  to  show  higher  hardness  than  other  alloys  could  be  due to  its  higher  density.  The  higher  density  in  this  alloy  after  SPS  in comparison  to  the  other  alloys  could  be  due  to  its  BCC  structure, which  could  have  enhanced  diffusion  causing  better  sintering.  More analysis  in  this  regard  to  contribution  of  strength  coming  from various  factors  is  underway  [23].  The  alloys  containing  FCC  phase have  shown  lower  hardness  due  to  their  solid  solution  (no  ordered phase)  and  their  lower  density.  However,  it  is  interesting  note  that the  hardness  of  NiCoCrCuFe  is  signiﬁcantly  higher  than  the  as  cast NiCoCrCuFe  [3].   Further,  it  has  been  observed  that  addition  of  Cr  to an  alloy  system  increases  the  hardness,  possibly  due  to  the  formation  hard  sigma  phase  [20,21].  The  hardness  obtained  for  NiCoCrFe and  AlCoCrCuFe  HEAs  is  higher  than  the  mechanically  alloyed  and spark  plasma  sintered  CoCrFeNiTiAl  HEAs,  and  vacuum  arc  melted refractory  TaNbHfZrTi  HEA  [13,24].\\n',\n",
       " 'Nanostructured  AlCoCrCuFe  and  NiCoCrCuFe  HEAs  are  produced by  MA   in  metastable  form,  and  stable  microstructures  are  obtained after  SPS  at  900 ◦C.  Transformation  of  BCC  to  FCC  takes  place  when \\x0cS.  Praveen  et  al.  /  Materials  Science  and  Engineering  A  534 (2012) 83–  89 89\\n',\n",
       " 'Al  is  replaced  by  Ni  in  AlCoCrCuFe.  During  MA,   partial  dissolution of  Cu  in  AlCoCrCuFe  forms  a  minor  FCC  along  with  BCC  phase,  and partial  dissolution  of  Cr  in  NiCoCrCuFe  forms  a  minor  BCC  phase along  with  major  FCC  phase.  In  AlCoCrCuFe  after  SPS,  the  Cu-rich phase  precipitates  out,  major  BCC  transforms  into  an  ordered  BCC (B2)  phase,  and  a  minor  sigma  phase  forms.  A  detailed  characterization  of  phases  in  these  alloys  together  with  NiCoFe,  NiCoCrFe,  and NiCoCuFe  conﬁrms  the  formation  of  Cu  and  sigma  phases  in  NiCoCrCuFe  and  AlCoCrCuFe.  Conclusively,  it  has  been  shown  that  Cu precipitates  out  during  sintering  in  an  alloy  system  containing  Cu, and  Cr-rich  sigma  phase  forms  in  a  system  containing  Cr.  Nanocrystallinity  has  been  maintained  even  after  sintering  at  900 ◦C.  The AlCoCrCuFe  shows  very  high  hardness  of  770    10  HV  due  to  itsnanocrystalline  B2  structure.  Alloys  containing  Cr  exhibit  higher hardness  (due  to \\n',\n",
       " '  phase)  as  compared  to  other  alloys  systems.\\n',\n",
       " 'Clearly,  the  solid  solution  criteria  proposed  for  HEAs  needs  a  revision  to  account  for  enthalpy  of  mixing  between  elements  in  a  given alloy  system.\\n',\n",
       " 'Mater.  Trans.  36A  (2005)  881–893.\\n',\n",
       " 'Mater.  Trans.  36A  (2005)  1263–1271.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc77078-5719-400f-b57b-83880d4b60f0",
   "metadata": {},
   "source": [
    "Load a QA LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ccf0ec-fc13-4f15-bfd4-cab338d28c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0d1a8-52f7-4016-9d53-6686a0027d49",
   "metadata": {},
   "source": [
    "## First, we find all compositions talked about in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79003679-1db0-4175-b181-a3c593517345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function to post process answers\n",
    "def postprocess_answers(ans):\n",
    "    # Remove extra white-spaces (makes it hard to get exact matches)\n",
    "    return re.sub(\"[ ]{2,}\", \" \", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf85f825-788f-4290-b1b2-b0f21f8d7d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 64.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 42.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 66.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 42.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction', 'Ni-based superalloys', 'high entropy', 'mechanical', 'AlCoCrCuFe', 'NiCoCuFe', 'NiCoCrCuFe', 'AlCoCrCuFe', 'nanocrystalline', 'AlCoCrCuFe', 'Experimental details', 'AlCoCrCuFe', 'NiCoFe HEAs', '3. Results', 'pellets', 'AlCoCrCuFe and NiCoCrCuFe', 'NiCoCrCuFe', 'NiCoFe, NiCoCrFe and NiCoCuFe together with NiCoCrCrFe', 'NiCoCrFe', 'AlCoCrCuFe', 'AlCoCrCuFe', 'NiCoCrCuFe', 'NiCoCuFe, NiCoCrCuFe, AlCoCrCuFe', 'HEAs', 'AlCoCrCuFe', 'NiCoCrFe', '83– 89 Table 1', 'MA', 'Crystallite', 'MA', 'Cu', 'BCC', 'Cu rich FCC', 'sigma', 'Relative density obtained for the SPS samples.', 'Vickers', 'NiCoCrCuFe', 'NiCoCrCuFe', 'AlCoCrCuFe', 'NiCoCrCuFe', 'XRD pattern (which is not clearly visible in the SEM images).', 'AlCoCrCuFe', 'Crystallite size and Vickers hardness', 'AlCoCrCuFe', 'AlCoCrCuFe', 'NiCoCrCuFe', 'NiCoCuFe', 'equi-atomic HEAs', '4.6%),', '83– 89 87', 'NiCoCrCuFe', 'AlCoCrCuFe', 'HEAs.', 'Enthalpy of mixing (kJ/mol) between elements.', 'Smix', 'NiCoFe', 'Cu', 'equi-atomic binary pairs of elements used in this study.', 'Al0.5CoCrCuFeNiTix', 'AlCoCrCuFe', 'NiCoCrCuFe', 'Ni', 'AlCoCrCuFe', 'Fe/Cr', 'NiCoCrCuFe', 'NiCoCrCuFe', 'AlCoCrCuFe', 'NiCoCrCuFe', 'AlCoCrCuFe', 'NiCoCrCuFe and AlCoCrCuFe', 'phase)', 'HEAs needs a revision to account for enthalpy of mixing between elements in a given alloy system.', '36A (2005) 881–893', '36A (2005) 1263–1271.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_list = []\n",
    "\n",
    "for para in paragraphs:\n",
    "    answers = reader.predict(\n",
    "        query=\"Which multicomponent alloy is discussed? For example: AuAgFeCu\",\n",
    "        documents=[Document(content=para)],\n",
    "        top_k=10\n",
    "    )\n",
    "    # print(\" || \".join([f\"{x.answer}({round(x.score, 2)})\" for x in answers[\"answers\"]]))\n",
    "    if len(answers[\"answers\"]) > 0:\n",
    "        ans = answers[\"answers\"][0].answer\n",
    "        ans = postprocess_answers(ans)\n",
    "        temp_list.append(ans)\n",
    "\n",
    "print(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49357038-c901-45be-bd08-4e0a917da869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlCoCrCuFe', 'NiCoCuFe', 'NiCoCrCuFe', 'NiCoCrFe', 'NiCoFe']\n"
     ]
    }
   ],
   "source": [
    "list_of_alloys = []\n",
    "\n",
    "for alloy in temp_list:\n",
    "    isalloy = re.search(r\"([A-Z][a-z][0-9\\.]*){2,}\", alloy)\n",
    "    if isalloy is not None and alloy == isalloy.group(0) and alloy not in list_of_alloys:\n",
    "        list_of_alloys.append(alloy)\n",
    "\n",
    "print(list_of_alloys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41064e0e-ebf2-4608-a7b4-5f26d92ecc2f",
   "metadata": {},
   "source": [
    "## Now, next we want to extract processing condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1561bdec-2453-4b84-9486-3c481292da1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['mechanical alloying', 'equi-atomic alloys',\n",
       "       'mechanically alloyed', 'HEAs',\n",
       "       'mechanically alloyed powders of AlCoCrCuFe and NiCoCrCuFe as a function of milling time',\n",
       "       'milling time', 'sintering', 'Cu', 'deconvoluted', 'phase',\n",
       "       'lattice parameter', 'MA in metastable form'], dtype='<U333')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alloy = list_of_alloys[1]\n",
    "list_of_answers = []\n",
    "list_of_scores = []\n",
    "\n",
    "for para in paragraphs:\n",
    "    answers = reader.predict(\n",
    "        # query=f\"What is the preparation method of {alloy}?\",\n",
    "        query=f\"How is {alloy} made?\", #+ \" among as-cast, additive manufacturing, Bridgman solidification, cold-pressed, mechanically alloyed plus spark plasma sintered, melt-spun, sputter deposition, splat quenched?\",\n",
    "        documents=[Document(content=para)],\n",
    "        top_k=10\n",
    "    )\n",
    "    # print(\" || \".join([f\"{x.answer}({round(x.score, 2)})\" for x in answers[\"answers\"]]))\n",
    "    if len(answers[\"answers\"]) > 0:\n",
    "        list_of_answers.extend(list(map(lambda x: postprocess_answers(x.answer), answers[\"answers\"])))\n",
    "        list_of_scores.extend(list(map(lambda x: x.score, answers[\"answers\"])))\n",
    "\n",
    "# Remove the answers with \"no confidence\"\n",
    "list_of_answers = np.array(list_of_answers)[np.array(list_of_scores) > 0.1]\n",
    "list_of_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb037569-bff7-4841-9458-df1e527f343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_processing = {\n",
    "    \"AC\": [\"cast\", \"AC\"],\n",
    "    \"MA\": [\"mechanical\", \"MA\"],\n",
    "}\n",
    "\n",
    "# For testing\n",
    "# for item in sum(dict_of_processing.values(), []):\n",
    "#     temp_l = list(filter(lambda x: item in x, list_of_answers))\n",
    "#     print(item, len(temp_l), temp_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d467b536-d34e-4086-93d5-9b5f0390bbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MA'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some voting....\n",
    "def get_num_answers_containing_search(answers, list_of_search_items):\n",
    "    return len(set(sum([list(filter(lambda x: item in x, answers)) for item in list_of_search_items], [])))\n",
    "\n",
    "def get_highest_voted_item_from_dict_in_answers(answers, dicty):\n",
    "    new_dict = {}\n",
    "    for key in dicty:\n",
    "        new_dict[key] = get_num_answers_containing_search(answers, dicty[key])\n",
    "    # print(new_dict)\n",
    "    keys = list(new_dict.keys())\n",
    "    values = [new_dict[key] for key in new_dict]\n",
    "    return keys[np.argmax(values)]\n",
    "\n",
    "get_highest_voted_item_from_dict_in_answers(list_of_answers, dict_of_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b7982-2ac2-4aab-9c29-ddb8ca742688",
   "metadata": {},
   "source": [
    "Now, SS or IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afa16793-d6aa-4f40-af99-2c471b422c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.14 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.20 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ni-based  superalloys(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 42.15 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equi-molar(0.05) || solid  solutions  under  appropriate  conditions(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 64.50 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAs(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.64 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equiatomic(0.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.61 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equi-atomic(0.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 64.72 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10   HV   for   NiCoCrCuFe.   Phase   evolution   after   MA   and   SPS   indicate (cid:2)that   conﬁgurational   entropy   is   not   sufﬁcient   enough   to  suppress   the   formation   of   Cu-rich   FCC,   and   phases,   and   enthalpy   of   mixing   appears   to   play   an   important   role   in   determining   the   phase   formation   in high   entropy   alloys   after   sintering.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 64.13 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solid(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.24 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metastable(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 42.50 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equi-atomic(0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 66.53 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental  details(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 74.10 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equi-atomic(0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.93 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAs(0.01) || ◦C  for  15  min   at  a  pressure of  50  MPa.  To  study  the  alloying  behavior  during  milling,  powder samples  were  taken  at  a  regular  interval  of  5  h  and  XRD  experiments  were  carried  out  in  Xpert  Pro  Panalytical  instrument.  Phase evolution  after  sintering  was  also  studied  by  XRD.  Hardness  measurements  were  carried  out  on  the  consolidated  samples  at  a  load of  1  kg  with  dwell  time  of  10  s  in  Wilpert  WilsonVicker’s  hardness instrument.  The  reported  hardness  measurements  are  an  average of  at  least  ten  measurements  done  on  both  sides  of  the  sample pellet.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.  Results(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pellets(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.56 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlCoCrCuFe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu(0.01) || NiCoCrFe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.89 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe  or  Cr(0.0) || NiCoCuFe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAs(0.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlCoCrCuFe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NiCoCrFe(0.01) || AlCoCrCuFe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloyed  HEAs(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlCoCrCuFe  and  NiCoCrCuFe  after  SPS(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NiCoFe(0.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.57 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table  1(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.82 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA   and  SPS  based  on  XRD  analysis.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crystallite  size  and  lattice  strain  of  MA   and  SPS  samples.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.83 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCC  (Bminor(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.42 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu  rich FCC(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.17 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma  phase  ((cid:2)) Two   FCC  (F1 and  F2) FCC  (F)(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.15 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative  density  obtained  for  the  SPS  samples.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.42 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAs.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.51 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr(0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloy(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equi-atomic(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.07 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr-rich(0.01) || equi-atomic  percentage  in  Cr  rich  phase.  At  lower  magniﬁcation,  as  shown  in  Fig.  5c,  Cr-depleted  light  grey  phase  seems  to  be distributed  as  a  network  like  structure.  However,  at  higher  magniﬁcation  (Fig.  5d),  Cr-rich(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.89 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XRD  pattern(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ni(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crystallite  size  and  Vickers  hardness(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.21 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlCoCrCuFe(0.01) || Crystallite  sizes  of 6–10  nm  and  lattice  strain  of  0.5–1.0%  are  observed  for  mechanical  alloyed  powders.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ni(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.79 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.37 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  HV  for  NiCoCuFe  and  230  4.  Discussion(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.74 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single  phase solid(0.0) || equi-atomic  proportion(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.80 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solid  solution(0.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83–  89 87(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.61 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlCoCrCuFe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.50 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlCoCrCuFe(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.19 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solid(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.59 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enthalpy  of  mixing  (kJ/mol)  between  elements.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.32 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy  of  mixing ((cid:2)Smix)(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.31 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solid(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table  6  presents  the \f",
      "88(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.25 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate(0.02) || solid  solution criteria  for  HEAs  [15,16],   simple  FCC  phase  or  BCC  phase  with  no intermediate  phase  is  expected  to  form  in  the  present  alloy  systems.  However,  in  addition  to  Cu-rich  phase  precipitation  during densiﬁcation,  sigma  phase  is  also  observed  in  all  the  alloy  systems containing  Cr.  This  sigma  phase  could  be  CoCr  or  FeCr(0.0) || Al0.5CoCrCuFeNiTix(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.54 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermetallic(0.03) || solid(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ﬁ(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.76 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloy(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NiCoCrFe(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.58 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe(0.0) || Fe(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.74 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe–Ni–Cr  FCC(0.01) || solid(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.83 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloys(0.0) || AlCoCrCuFe.  In  this  connection,  it  is  important  to  note  that  FCC  is  the  major  phase  in  all  the  systems  except in  AlCoCrCuFe.  Crystal  growth  or  grain  growth  is  governed  by  diffusion  kinetics  in  a  given  system.  In  general,  BCC  being  an  open structure,  diffusion  takes  place  easily  as  compared  to  close  packed structure  of  FCC.  Thus,  crystal  growth  in  Ni  containing  alloys(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.42 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solid  solution(0.16) || AlCoCrCuFe  HEAs  is  higher  than  the  mechanically  alloyed  and spark  plasma  sintered  CoCrFeNiTiAl  HEAs(0.0) || nanocrystallinity  in  AlCoCrCuFe  must  be responsible  for  exhibiting  higher  hardness.  One  of  the  reasons  for this  alloy  to  show  higher  hardness  than  other  alloys  could  be  due to  its  higher  density(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.43 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanostructured(0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.12 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NiCoCrFe(0.01) || nanocrystalline  B2  structure(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.84 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase)  as  compared  to  other  alloys  systems.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solid(0.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881–893(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263–1271.(0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "alloy = list_of_alloys[2]\n",
    "list_of_answers = []\n",
    "list_of_scores = []\n",
    "\n",
    "for para in paragraphs:\n",
    "    answers = reader.predict(\n",
    "        # query=f\"What is the preparation method of {alloy}?\",\n",
    "        query=f\"{alloy} is solid solution, intermetallic or amorphous?\", #+ \" among as-cast, additive manufacturing, Bridgman solidification, cold-pressed, mechanically alloyed plus spark plasma sintered, melt-spun, sputter deposition, splat quenched?\",\n",
    "        documents=[Document(content=para)],\n",
    "        top_k=10\n",
    "    )\n",
    "    print(\" || \".join([f\"{x.answer}({round(x.score, 2)})\" for x in answers[\"answers\"]]))\n",
    "    if len(answers[\"answers\"]) > 0:\n",
    "        list_of_answers.extend(list(map(lambda x: postprocess_answers(x.answer), answers[\"answers\"])))\n",
    "        list_of_scores.extend(list(map(lambda x: x.score, answers[\"answers\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d869c82a-16b3-43d4-a768-e869050792c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['solid solution', 'solid solution'], dtype='<U547')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_answers = np.array(list_of_answers)[np.array(list_of_scores) > 0.1]\n",
    "list_of_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43728e37-0900-4270-8283-87dd7b97be79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SS'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some voting again... \n",
    "dict_of_SS_IM = {\n",
    "    \"SS\": [\"solid solution\", \"solid-solution\"],\n",
    "    \"IM\": [\"intermetallic\"]\n",
    "}\n",
    "\n",
    "get_highest_voted_item_from_dict_in_answers(list_of_answers, dict_of_SS_IM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dda4c-abe8-4ad2-be37-2ddf15a8cc3c",
   "metadata": {},
   "source": [
    "Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d2ae53-6aeb-4d81-93e2-66e889b1f9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "alloy = list_of_alloys[1]\n",
    "list_of_answers = []\n",
    "list_of_scores = []\n",
    "\n",
    "for para in paragraphs:\n",
    "    answers = reader.predict(\n",
    "        # query=f\"What is phase of {alloy}?\", \n",
    "        query=f\"Is {alloy} FCC or BCC?\", \n",
    "        documents=[Document(content=para)],\n",
    "        top_k=10\n",
    "    )\n",
    "    # print(\" || \".join([f\"{x.answer}({round(x.score, 2)})\" for x in answers[\"answers\"]]))\n",
    "    if len(answers[\"answers\"]) > 0:\n",
    "        list_of_answers.extend(list(map(lambda x: postprocess_answers(x.answer), answers[\"answers\"])))\n",
    "        list_of_scores.extend(list(map(lambda x: x.score, answers[\"answers\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aec72e2-c850-4c2e-965c-a6722304a89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NiCoCuFe', 'BCC', 'BCC', 'FCC', 'FCC', 'FCu', 'BCC', 'BCC', 'BCC',\n",
       "       'FCC', 'BCC', 'FCC', 'FCC', 'BCC', 'BCC', 'BCC', 'BCC', 'BCC'],\n",
       "      dtype='<U270')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_answers = np.array(list_of_answers)[np.array(list_of_scores) > 0.1]\n",
    "list_of_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe434cb-ccb4-434e-b00c-8c04681bc20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SS'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some voting again... \n",
    "dict_of_phases = {\n",
    "    \"BCC\": [\"BCC\"],\n",
    "    \"FCC\": [\"FCC\"]\n",
    "}\n",
    "\n",
    "get_highest_voted_item_from_dict_in_answers(list_of_answers, dict_of_SS_IM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a94da-eb72-4a5c-9cef-3abb53c0db27",
   "metadata": {},
   "source": [
    "## Property explored in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c5b084-a7a2-4d72-9d1f-acc1cb368ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['high entropy', '99.5%', 'Hardness', 'hardness', 'XRD patterns',\n",
       "       'XRD', 'milling time', 'XRD peaks', 'FCC phases', 'segregation',\n",
       "       'de-convoluted peaks',\n",
       "       'XRD patterns of AlCoCrCuFe and NiCoCrCuFe after SPS. Inset shows the deconvoluted form of XRD peaks',\n",
       "       'de-convoluted form of XRD peaks', 'Crystallite size',\n",
       "       'sigma phase', 'density', 'relative density', 'Spot EDS analysis',\n",
       "       'experimental density', 'Crystallite size and lattice strain',\n",
       "       'Williamson–Hall analysis', 'hardness', 'entropy',\n",
       "       'parameters are calculated for the alloy systems',\n",
       "       'BSE-SEM images of SPS pellets', 'kJ/mol', 'enthalpy',\n",
       "       'sigma phase', 'entropy', 'a shift in the Ni peaks',\n",
       "       '3.52 and 3.61 ˚A', '3.58, 3.60, 3.59 and 3.62 ˚A', 'density'],\n",
       "      dtype='<U318')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_answers = []\n",
    "list_of_scores = []\n",
    "\n",
    "for para in paragraphs:\n",
    "    answers = reader.predict(\n",
    "        query=\"What is measured?\", # Alloy agnostic\n",
    "        documents=[Document(content=para)],\n",
    "        top_k=10\n",
    "    )\n",
    "    # print(\" || \".join([f\"{x.answer}({round(x.score, 2)})\" for x in answers[\"answers\"]]))\n",
    "    if len(answers[\"answers\"]) > 0:\n",
    "        list_of_answers.extend(list(map(lambda x: postprocess_answers(x.answer), answers[\"answers\"])))\n",
    "        list_of_scores.extend(list(map(lambda x: x.score, answers[\"answers\"])))\n",
    "\n",
    "list_of_answers = np.array(list_of_answers)[np.array(list_of_scores) > 0.1]\n",
    "list_of_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92a04f5e-a7c4-49ba-8457-46ac6c0f5d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SS'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some voting again... \n",
    "dict_of_properties = {\n",
    "    \"H\": [\"hardness\", \"Hardness\"],\n",
    "    \"M\": [\"Mechanical\", \"microstructure\", \"strength\", \"compress\"]\n",
    "}\n",
    "\n",
    "get_highest_voted_item_from_dict_in_answers(list_of_answers, dict_of_SS_IM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0bc2e-825c-4c88-aeac-732b1d23a772",
   "metadata": {},
   "source": [
    "# LET'S MAKE THAT TABLE NOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8aef4962-e70c-4b37-8dfa-b073a464c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_paragraphs(file):\n",
    "    return open(file, 'r').readlines()\n",
    "\n",
    "# Small function to post process answers\n",
    "def postprocess_answers(ans):\n",
    "    # Remove extra white-spaces (makes it hard to get exact matches)\n",
    "    return re.sub(\"[ ]{2,}\", \" \", ans)\n",
    "\n",
    "def run_query_on_paragraphs(model, paragraphs, query, threshold=0.1):\n",
    "    '''\n",
    "    model: QA model\n",
    "    paragraphs: list of strings to run query on\n",
    "    query: string to use for QA model's query\n",
    "    threshold: Answers with confidence above this threshold will be returned\n",
    "    '''\n",
    "    list_of_answers = []\n",
    "    list_of_scores = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        answers = model.predict(\n",
    "            query=query,\n",
    "            documents=[Document(content=para)],\n",
    "            top_k=10\n",
    "        )\n",
    "        # print(\" || \".join([f\"{x.answer}({round(x.score, 2)})\" for x in answers[\"answers\"]]))\n",
    "        if len(answers[\"answers\"]) > 0:\n",
    "            list_of_answers.extend(list(map(lambda x: x.answer, answers[\"answers\"])))\n",
    "            list_of_scores.extend(list(map(lambda x: x.score, answers[\"answers\"])))\n",
    "    \n",
    "    # Remove the answers with \"no confidence\"\n",
    "    list_of_answers = np.array(list_of_answers)[np.array(list_of_scores) > 0.1]\n",
    "    return list_of_answers\n",
    "\n",
    "def filter_alloy_compositions(l):\n",
    "    '''\n",
    "    takes a list of string and returns those that have an alloy composition\n",
    "    '''\n",
    "    list_of_alloys = []\n",
    "\n",
    "    for alloy in l:\n",
    "        isalloy = re.search(r\"([A-Z][a-z][0-9\\.x]*){2,}\", alloy)\n",
    "        if isalloy is not None and alloy == isalloy.group(0) and alloy not in list_of_alloys:\n",
    "            list_of_alloys.append(alloy)\n",
    "    \n",
    "    return list_of_alloys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5687bacb-106e-45c9-8004-53f1ad45b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 s, sys: 451 ms, total: 3.52 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from haystack.nodes import FARMReader\n",
    "\n",
    "%time reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8d36021-3913-403a-a1ea-5323e45d328c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_6.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference_txts/reference_6.txt\n",
      "Paragraphs: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 28.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 22.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 66.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_82.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n",
      "reference_txts/reference_82.txt\n",
      "Paragraphs: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_101.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n",
      "reference_txts/reference_101.txt\n",
      "Paragraphs: 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_5.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 109\n",
      "reference_txts/reference_5.txt\n",
      "Paragraphs: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_113.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "reference_txts/reference_113.txt\n",
      "Paragraphs: 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 73.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 49.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_102.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 138\n",
      "reference_txts/reference_102.txt\n",
      "Paragraphs: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.29 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_112.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38\n",
      "reference_txts/reference_112.txt\n",
      "Paragraphs: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_30.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "reference_txts/reference_30.txt\n",
      "Paragraphs: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 36.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 71.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 25.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 74.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 73.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 38.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 53.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_89.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 102\n",
      "reference_txts/reference_89.txt\n",
      "Paragraphs: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 38.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 53.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 56.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 73.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.81 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 69.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 71.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_86.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 67\n",
      "reference_txts/reference_86.txt\n",
      "Paragraphs: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_46.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n",
      "reference_txts/reference_46.txt\n",
      "Paragraphs: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_26.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n",
      "reference_txts/reference_26.txt\n",
      "Paragraphs: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_53.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n",
      "reference_txts/reference_53.txt\n",
      "Paragraphs: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 72.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 68.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_56.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n",
      "reference_txts/reference_56.txt\n",
      "Paragraphs: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_114.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "reference_txts/reference_114.txt\n",
      "Paragraphs: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_28.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24\n",
      "reference_txts/reference_28.txt\n",
      "Paragraphs: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_93.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 66\n",
      "reference_txts/reference_93.txt\n",
      "Paragraphs: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 75.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 66.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 71.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 54.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.35 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.03 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_70.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "reference_txts/reference_70.txt\n",
      "Paragraphs: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 38.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_11.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n",
      "reference_txts/reference_11.txt\n",
      "Paragraphs: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_25.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n",
      "reference_txts/reference_25.txt\n",
      "Paragraphs: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 71.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_105.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n",
      "reference_txts/reference_105.txt\n",
      "Paragraphs: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 25.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 69.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_99.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 118\n",
      "reference_txts/reference_99.txt\n",
      "Paragraphs: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 97.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_35.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35\n",
      "reference_txts/reference_35.txt\n",
      "Paragraphs: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_36.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n",
      "reference_txts/reference_36.txt\n",
      "Paragraphs: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_33.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 76\n",
      "reference_txts/reference_33.txt\n",
      "Paragraphs: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_100.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n",
      "reference_txts/reference_100.txt\n",
      "Paragraphs: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_111.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35\n",
      "reference_txts/reference_111.txt\n",
      "Paragraphs: 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 69.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 73.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 72.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 48.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_72.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 121\n",
      "reference_txts/reference_72.txt\n",
      "Paragraphs: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.26 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_47.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n",
      "reference_txts/reference_47.txt\n",
      "Paragraphs: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_1.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 73\n",
      "reference_txts/reference_1.txt\n",
      "Paragraphs: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.37 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 73.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.43 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 53.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_44.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n",
      "reference_txts/reference_44.txt\n",
      "Paragraphs: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.15 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.43 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 38.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_73.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n",
      "reference_txts/reference_73.txt\n",
      "Paragraphs: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_14.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n",
      "reference_txts/reference_14.txt\n",
      "Paragraphs: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.74 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.99 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.93 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_59.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n",
      "reference_txts/reference_59.txt\n",
      "Paragraphs: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_50.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n",
      "reference_txts/reference_50.txt\n",
      "Paragraphs: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_68.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 89\n",
      "reference_txts/reference_68.txt\n",
      "Paragraphs: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 22.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.57 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 25.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 21.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.77 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_117.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11\n",
      "reference_txts/reference_117.txt\n",
      "Paragraphs: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_92.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n",
      "reference_txts/reference_92.txt\n",
      "Paragraphs: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 51.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 56.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 55.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.36 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 67.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 46.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 49.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.20 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 56.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 71.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 56.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 45.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 67.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 53.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_58.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 148\n",
      "reference_txts/reference_58.txt\n",
      "Paragraphs: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.40 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_43.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31\n",
      "reference_txts/reference_43.txt\n",
      "Paragraphs: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.27 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.09 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_91.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n",
      "reference_txts/reference_91.txt\n",
      "Paragraphs: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.34 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_45.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n",
      "reference_txts/reference_45.txt\n",
      "Paragraphs: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_3.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60\n",
      "reference_txts/reference_3.txt\n",
      "Paragraphs: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.28 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00,  4.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 97.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_29.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 84\n",
      "reference_txts/reference_29.txt\n",
      "Paragraphs: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_104.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n",
      "reference_txts/reference_104.txt\n",
      "Paragraphs: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.17 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.47 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.92 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_106.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "reference_txts/reference_106.txt\n",
      "Paragraphs: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_69.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 85\n",
      "reference_txts/reference_69.txt\n",
      "Paragraphs: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_119.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "reference_txts/reference_119.txt\n",
      "Paragraphs: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.31 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_64.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n",
      "reference_txts/reference_64.txt\n",
      "Paragraphs: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.22 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 74.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.42 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.00 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.98 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.76 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_48.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 58\n",
      "reference_txts/reference_48.txt\n",
      "Paragraphs: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 74.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_54.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 57\n",
      "reference_txts/reference_54.txt\n",
      "Paragraphs: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 73.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.14 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 97.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_57.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n",
      "reference_txts/reference_57.txt\n",
      "Paragraphs: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 56.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 31.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 67.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.94 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 67.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_95.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n",
      "reference_txts/reference_95.txt\n",
      "Paragraphs: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.12 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_76.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 65\n",
      "reference_txts/reference_76.txt\n",
      "Paragraphs: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_38.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n",
      "reference_txts/reference_38.txt\n",
      "Paragraphs: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_103.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 43\n",
      "reference_txts/reference_103.txt\n",
      "Paragraphs: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.69 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_27.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 59\n",
      "reference_txts/reference_27.txt\n",
      "Paragraphs: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 71.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 81.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.80 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 68.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_87.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n",
      "reference_txts/reference_87.txt\n",
      "Paragraphs: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 63.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.07 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 97.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 97.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_17.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 61\n",
      "reference_txts/reference_17.txt\n",
      "Paragraphs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.60 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 82.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.54 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.32 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.97 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_4.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "reference_txts/reference_4.txt\n",
      "Paragraphs: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.16 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_63.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 34\n",
      "reference_txts/reference_63.txt\n",
      "Paragraphs: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.35 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_109.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "reference_txts/reference_109.txt\n",
      "Paragraphs: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.11 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.78 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.46 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.87 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 96.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.53 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 84.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 39.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.33 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 95.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 26.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.14 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 25.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.25 Batches/s]\n",
      "/tmp/ipykernel_2484151/3534423196.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='reference_txts/reference_31.txt' mode='r' encoding='UTF-8'>\n",
      "  return open(file, 'r').readlines()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n",
      "reference_txts/reference_31.txt\n",
      "Paragraphs: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.66 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.02 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.13 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.25 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 94.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.92 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.04 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.05 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 33.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.31 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.77 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.89 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 75.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 76.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 79.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.89 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 80.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.21 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.24 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.39 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.55 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.73 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.44 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.64 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.98 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.65 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.61 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.40 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.38 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 83.81 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.84 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.30 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.67 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.70 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.33 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.71 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.76 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.86 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.96 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.72 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.28 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.28 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 40.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.87 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.07 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.49 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.04 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.79 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.15 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.26 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.57 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.10 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.11 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.47 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.00 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.27 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.99 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.53 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.68 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.12 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.97 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 61.34 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.43 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.63 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.35 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 41.29 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.54 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.42 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 93.59 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.60 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 85.09 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.58 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 32.78 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.32 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.45 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.83 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.52 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.16 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.66 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 91.75 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.36 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 78.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.62 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.80 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.88 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.69 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.34 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.74 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.93 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.24 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 92.94 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.18 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 87.91 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 90.48 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 86.14 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.56 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.37 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 88.85 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.17 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 89.89 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = [\"Composition\", \"Processing Condition\", \"SS or IM\", \"Phases\", \"Property\", \"References\"])\n",
    "\n",
    "for file in glob.glob(\"reference_txts/*\"):\n",
    "    # print(file)\n",
    "    ref = int(re.findall(\"reference_([0-9]+).txt\", file)[0])\n",
    "    \n",
    "    # Get content from reference\n",
    "    paragraphs = get_paragraphs(file) \n",
    "    # print(\"Paragraphs:\", len(paragraphs))\n",
    "\n",
    "    # Get alloys\n",
    "    answers = run_query_on_paragraphs(reader, paragraphs,\n",
    "        query=\"Which multicomponent alloy is discussed? For example: AuAgFeCu\", threshold=0)\n",
    "    list_of_alloys = filter_alloy_compositions(answers)\n",
    "    # assert len(list_of_alloys) > 0, f\"File = {file}\"\n",
    "\n",
    "    # Get the property (global)\n",
    "    answers = run_query_on_paragraphs(reader, paragraphs, \"What is measured?\")\n",
    "    pr = get_highest_voted_item_from_dict_in_answers(answers, dict_of_properties)\n",
    "    \n",
    "    # For each alloy, get other details\n",
    "    for alloy in list_of_alloys:\n",
    "        # preprocessing condition\n",
    "        answers = run_query_on_paragraphs(reader, paragraphs, f\"How is {alloy} made?\")\n",
    "        pc = get_highest_voted_item_from_dict_in_answers(answers, dict_of_processing)\n",
    "\n",
    "        # SS or IM\n",
    "        answers = run_query_on_paragraphs(reader, paragraphs, f\"{alloy} is solid solution, intermetallic or amorphous?\")\n",
    "        si = get_highest_voted_item_from_dict_in_answers(answers, dict_of_SS_IM)\n",
    "\n",
    "        # Phases\n",
    "        answers = run_query_on_paragraphs(reader, paragraphs, f\"Is {alloy} FCC or BCC?\")\n",
    "        ph = get_highest_voted_item_from_dict_in_answers(answers, dict_of_phases)\n",
    "\n",
    "        results.loc[results.shape[0] + 1] = [alloy, pc, si, ph, pr, ref]\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10031e88-bef5-4e7d-ada5-e6763bb6dd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Composition</th>\n",
       "      <th>Processing Condition</th>\n",
       "      <th>SS or IM</th>\n",
       "      <th>Phases</th>\n",
       "      <th>Property</th>\n",
       "      <th>References</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlCoCrCuFeNiMox</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LaNi5</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ta20Nb20Hf20Zr20Ti20</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FeNiCr</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fe20Cr20Mn20Ni20Co20</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>M</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AlCrFeCoNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AlxCoCrFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AlCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CoCrFeNiMox</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AlxCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AlCoCrCuFeNiMox</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AlCoCrFeNiMox</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AlxCoFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SixCoFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AlxCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Al23Co15Cr23Cu8Fe15Ni16</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CuNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Al0.4Hf0.6NbTaTiZr</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AlMo0.5NbTa0.5TiZr</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FeMnNiCuCoSnx</td>\n",
       "      <td>AC</td>\n",
       "      <td>IM</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AlCoCrCuNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AlAuCoCrCuNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AlxCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AgAlCoCrCuNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AlxCoCrFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AlxCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AlLiMgZnSn</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>M</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CrFeMnNiAlx</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CoCrFeMnNiVx</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CoCrFeNiMnVx</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AlCoCrFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AlCoCrFeNiCx</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AlCoCrCuFe</td>\n",
       "      <td>MA</td>\n",
       "      <td>IM</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NiCoCuFe</td>\n",
       "      <td>MA</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NiCoFe</td>\n",
       "      <td>MA</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>FeCo</td>\n",
       "      <td>AC</td>\n",
       "      <td>IM</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>SiNiCrTiAl</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GdTbDyTmLu</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>CoCrFeNiTiAl</td>\n",
       "      <td>AC</td>\n",
       "      <td>IM</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AlxCoCrFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>FCC</td>\n",
       "      <td>H</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>AlCr2FeCoNi</td>\n",
       "      <td>MA</td>\n",
       "      <td>IM</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AlCoCrFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>IM</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AlCoCrFeNiSix</td>\n",
       "      <td>AC</td>\n",
       "      <td>IM</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CoCrFeNiTi0.3</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MnCr</td>\n",
       "      <td>MA</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>CoCrFeNiCuAl</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fe40Mn40Co10Cr10</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>AlxCoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Al0.5CoCrCuFeNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>CoCrFeNiTiAlx</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>AlCoCrFeNiMox</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>AlxCoCrCuFeNi</td>\n",
       "      <td>MA</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>M</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>FeCoNiCuAl</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>AlCoCrCuNi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>FeCoNiCuAlSi</td>\n",
       "      <td>AC</td>\n",
       "      <td>SS</td>\n",
       "      <td>BCC</td>\n",
       "      <td>H</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Composition Processing Condition SS or IM Phases Property  \\\n",
       "1           AlCoCrCuFeNiMox                   AC       SS    BCC        M   \n",
       "2                     LaNi5                   AC       SS    BCC        H   \n",
       "3              AlCoCrCuFeNi                   AC       SS    BCC        H   \n",
       "4      Ta20Nb20Hf20Zr20Ti20                   AC       SS    BCC        M   \n",
       "5                    FeNiCr                   AC       SS    FCC        H   \n",
       "6      Fe20Cr20Mn20Ni20Co20                   AC       SS    FCC        M   \n",
       "7                AlCrFeCoNi                   AC       SS    BCC        M   \n",
       "8               AlxCoCrFeNi                   AC       SS    BCC        H   \n",
       "9              AlCoCrCuFeNi                   AC       SS    BCC        H   \n",
       "10              CoCrFeNiMox                   AC       SS    BCC        H   \n",
       "11            AlxCoCrCuFeNi                   AC       SS    FCC        H   \n",
       "12          AlCoCrCuFeNiMox                   AC       SS    BCC        H   \n",
       "13            AlCoCrFeNiMox                   AC       SS    BCC        H   \n",
       "14                AlxCoFeNi                   AC       SS    BCC        H   \n",
       "15                SixCoFeNi                   AC       SS    BCC        H   \n",
       "16            AlxCoCrCuFeNi                   AC       SS    BCC        H   \n",
       "17  Al23Co15Cr23Cu8Fe15Ni16                   AC       SS    BCC        H   \n",
       "18                     CuNi                   AC       SS    BCC        H   \n",
       "19       Al0.4Hf0.6NbTaTiZr                   AC       SS    BCC        M   \n",
       "20       AlMo0.5NbTa0.5TiZr                   AC       SS    BCC        M   \n",
       "21            FeMnNiCuCoSnx                   AC       IM    FCC        H   \n",
       "22               AlCoCrCuNi                   AC       SS    BCC        H   \n",
       "23             AlAuCoCrCuNi                   AC       SS    BCC        H   \n",
       "24            AlxCoCrCuFeNi                   AC       SS    BCC        H   \n",
       "25             AgAlCoCrCuNi                   AC       SS    BCC        H   \n",
       "26              AlxCoCrFeNi                   AC       SS    BCC        H   \n",
       "27            AlxCoCrCuFeNi                   AC       SS    BCC        H   \n",
       "28               AlLiMgZnSn                   AC       SS    FCC        M   \n",
       "29              CrFeMnNiAlx                   AC       SS    FCC        H   \n",
       "30             CoCrFeMnNiVx                   AC       SS    FCC        H   \n",
       "31             CoCrFeNiMnVx                   AC       SS    FCC        H   \n",
       "32               AlCoCrFeNi                   AC       SS    BCC        M   \n",
       "33             AlCoCrFeNiCx                   AC       SS    BCC        M   \n",
       "34               AlCoCrCuFe                   MA       IM    BCC        H   \n",
       "35                 NiCoCuFe                   MA       SS    BCC        H   \n",
       "36                   NiCoFe                   MA       SS    BCC        H   \n",
       "37                     FeCo                   AC       IM    BCC        H   \n",
       "38               SiNiCrTiAl                   AC       SS    FCC        H   \n",
       "39               GdTbDyTmLu                   AC       SS    BCC        H   \n",
       "40             CoCrFeNiTiAl                   AC       IM    BCC        M   \n",
       "41              AlxCoCrFeNi                   AC       SS    FCC        H   \n",
       "42              AlCr2FeCoNi                   MA       IM    BCC        H   \n",
       "43               AlCoCrFeNi                   AC       IM    BCC        H   \n",
       "44            AlCoCrFeNiSix                   AC       IM    BCC        H   \n",
       "45            CoCrFeNiTi0.3                   AC       SS    BCC        M   \n",
       "46                     MnCr                   MA       SS    BCC        H   \n",
       "47             CoCrFeNiCuAl                   AC       SS    BCC        H   \n",
       "48         Fe40Mn40Co10Cr10                   AC       SS    BCC        M   \n",
       "49            AlxCoCrCuFeNi                   AC       SS    BCC        H   \n",
       "50          Al0.5CoCrCuFeNi                   AC       SS    BCC        H   \n",
       "51            CoCrFeNiTiAlx                   AC       SS    BCC        H   \n",
       "52            AlCoCrFeNiMox                   AC       SS    BCC        M   \n",
       "53            AlxCoCrCuFeNi                   MA       SS    BCC        M   \n",
       "54               FeCoNiCuAl                   AC       SS    BCC        H   \n",
       "55               AlCoCrCuNi                   AC       SS    BCC        H   \n",
       "56             FeCoNiCuAlSi                   AC       SS    BCC        H   \n",
       "\n",
       "    References  \n",
       "1            6  \n",
       "2          101  \n",
       "3          102  \n",
       "4          112  \n",
       "5           89  \n",
       "6           86  \n",
       "7           53  \n",
       "8           56  \n",
       "9           28  \n",
       "10          93  \n",
       "11          93  \n",
       "12          93  \n",
       "13          93  \n",
       "14          70  \n",
       "15          70  \n",
       "16          11  \n",
       "17          11  \n",
       "18          25  \n",
       "19         105  \n",
       "20         105  \n",
       "21          99  \n",
       "22           1  \n",
       "23           1  \n",
       "24           1  \n",
       "25           1  \n",
       "26          44  \n",
       "27          14  \n",
       "28         117  \n",
       "29          92  \n",
       "30          92  \n",
       "31          92  \n",
       "32          58  \n",
       "33          58  \n",
       "34           3  \n",
       "35           3  \n",
       "36           3  \n",
       "37          29  \n",
       "38         104  \n",
       "39         119  \n",
       "40          64  \n",
       "41          48  \n",
       "42          54  \n",
       "43          57  \n",
       "44          57  \n",
       "45          38  \n",
       "46         103  \n",
       "47          27  \n",
       "48          87  \n",
       "49          17  \n",
       "50          17  \n",
       "51          63  \n",
       "52         109  \n",
       "53         109  \n",
       "54          31  \n",
       "55          31  \n",
       "56          31  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a7e7bf-fe10-4f12-8618-4837b7e97d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results/basic_QA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b849a-a62f-4ebb-b496-6b1efeb12b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03705d-54ea-4c09-b79d-7da813a0cfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84006b14-17b6-4812-9d34-04bd41a49136",
   "metadata": {},
   "source": [
    "Remember to track memory, especially GPU! Shut this notebook before running another!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
